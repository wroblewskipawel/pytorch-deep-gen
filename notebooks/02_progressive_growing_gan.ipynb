{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as tf\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.deepgen.models import progressive\n",
    "from python.deepgen.datasets import common\n",
    "from python.deepgen.utils import image, init\n",
    "from python.deepgen.training import adversarial, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "shuffle = True\n",
    "num_workers = 8\n",
    "num_lods = 4\n",
    "dataloaders = {}\n",
    "output_transforms = {}\n",
    "samples = []\n",
    "\n",
    "for lod in range(num_lods):\n",
    "    transform = tf.Compose([\n",
    "        tf.Resize((4*2**lod, 4*2**lod)),\n",
    "        tf.ToTensor(),\n",
    "    ])\n",
    "    cifar10n, cifar_mean, cifar_std = common.cifar10n(transform)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        cifar10n, batch_size, shuffle, num_workers=num_workers)\n",
    "    output_transform = tf.Compose([\n",
    "        image.Unnormalize(cifar_mean, cifar_std),\n",
    "        tf.Lambda(lambda x: x.squeeze(0)),\n",
    "        tf.ToPILImage('RGB')\n",
    "    ])\n",
    "    lod_samples = torch.split(next(iter(dataloader)), 1)[:4]\n",
    "    samples.extend([output_transform(s) for s in lod_samples])\n",
    "\n",
    "    dataloaders[lod+1] = dataloader\n",
    "    output_transforms[lod+1] = output_transform\n",
    "\n",
    "\n",
    "num_cols = 4\n",
    "num_rows = len(samples)//num_cols\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(2*num_cols, 2*num_rows))\n",
    "for sample, ax in zip(samples, axes.ravel()):\n",
    "    ax.imshow(sample)\n",
    "\n",
    "for ax in axes.ravel():\n",
    "    ax.axis('off')\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progressive GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "epoch_iters = len(dataloaders[list(dataloaders.keys())[-1]])*epochs\n",
    "fadeout_iters = epoch_iters // 2\n",
    "stable_iters = epoch_iters - fadeout_iters\n",
    "transition_iters = (fadeout_iters, stable_iters)\n",
    "\n",
    "init_map = {\n",
    "    nn.Conv2d: {'weight': (nn.init.kaiming_normal_, {'nonlinearity': 'leaky_relu'}),\n",
    "                'bias': (nn.init.constant_, {'val': 0.0})},\n",
    "    nn.ConvTranspose2d: {'weight': (nn.init.kaiming_normal_, {'nonlinearity': 'leaky_relu'}),\n",
    "                         'bias': (nn.init.constant_, {'val': 0.0})},\n",
    "}\n",
    "\n",
    "init_fn = init.Initializer(init_map)\n",
    "\n",
    "latent_dims = 128\n",
    "\n",
    "model, scheduler = progressive._progressive_gan_builder(\n",
    "    transition_iters, latent_dims, 3, 32, 2, 4, init_fn)\n",
    "\n",
    "model['gen'].to(device)\n",
    "model['disc'].to(device)\n",
    "\n",
    "gen_opt = optim.Adam(model['gen'].parameters(), 1e-3, betas=(0.0, 0.99), eps=1e-8)\n",
    "disc_opt = optim.Adam(model['disc'].parameters(), 1e-3, betas=(0.0, 0.99), eps=1e-8)\n",
    "optims = {'gen': gen_opt, 'disc': disc_opt}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterions = [loss.Wasserstein1()]\n",
    "reg = [loss.GradPenalty(10)]\n",
    "callbacks = {'iteration': [scheduler]}\n",
    "samplers = {\n",
    "    'latent': lambda bs: torch.randn(bs, latent_dims, 1, 1),\n",
    "    'labels': lambda bs: (torch.ones(bs, 1, 1, 1), (-1)*torch.ones(bs, 1, 1, 1))\n",
    "}\n",
    "trainer = adversarial.Trainer(model, optims, samplers, criterions, device, reg=reg, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lod_samples = {}\n",
    "output_dir = '../models/progressive_gan_cifar10/'\n",
    "for i, (dataloader, output_transform) in enumerate(\n",
    "        zip(dataloaders.values(),output_transforms.values())):\n",
    "    scale_output_dir = os.path.join(output_dir, f'{i+1}')\n",
    "    trainer.train(dataloader, epochs, scale_output_dir, output_transform, True)\n",
    "    sample_latent = samplers['latent'](16)\n",
    "    sample = model['gen'](sample_latent.to(device)).cpu()\n",
    "    lod_samples[i+1] = [output_transform(s)for s in torch.split(sample, 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('pytorch_env': conda)",
   "name": "python385jvsc74a57bd0e016d4dd0e064b7c1aff94752bda7a13d8e29c3d77200207baf8d0194d31e52e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "e016d4dd0e064b7c1aff94752bda7a13d8e29c3d77200207baf8d0194d31e52e"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}